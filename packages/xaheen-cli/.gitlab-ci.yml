# GitLab CI/CD Pipeline for Xaheen CLI
# Production-ready deployment with Norwegian compliance and security scanning

variables:
  DOCKER_IMAGE_TAG: $CI_REGISTRY_IMAGE:$CI_COMMIT_REF_SLUG-$CI_COMMIT_SHORT_SHA
  KUBERNETES_NAMESPACE: xaheen-cli
  HELM_CHART_VERSION: "1.0.0"
  NODE_VERSION: "20.11.0"
  SONAR_PROJECT_KEY: "xaheen-cli"
  VAULT_ADDR: $VAULT_URL
  NORWEGIAN_COMPLIANCE_LEVEL: "RESTRICTED"

stages:
  - validate
  - security-scan
  - test
  - build
  - package
  - deploy-dev
  - compliance-check
  - deploy-staging
  - security-validation
  - deploy-production
  - post-deployment

# Cache configuration for faster builds
cache:
  key: ${CI_COMMIT_REF_SLUG}
  paths:
    - node_modules/
    - .npm/
    - packages/*/node_modules/

# Global before script
before_script:
  - export PATH=$PATH:/usr/local/bin
  - echo "Pipeline started for commit $CI_COMMIT_SHORT_SHA"

# Stage 1: Validation and Linting
validate:
  stage: validate
  image: node:${NODE_VERSION}-alpine
  script:
    - npm ci --cache .npm --prefer-offline
    - npm run lint
    - npm run type-check
    - npm run validate:config
    - npm run validate:norwegian-compliance
  artifacts:
    reports:
      junit: test-results.xml
    paths:
      - coverage/
    expire_in: 1 week
  rules:
    - if: $CI_PIPELINE_SOURCE == "merge_request_event"
    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH

# Stage 2: Security Scanning
security-scan:
  stage: security-scan
  image: 
    name: aquasec/trivy:latest
    entrypoint: [""]
  parallel:
    matrix:
      - SCAN_TYPE: [fs, config, secret]
  script:
    - trivy --cache-dir .trivycache/ $SCAN_TYPE --exit-code 0 --format template --template "@contrib/sarif.tpl" -o trivy-$SCAN_TYPE-report.sarif .
    - trivy --cache-dir .trivycache/ $SCAN_TYPE --exit-code 1 --severity HIGH,CRITICAL .
  artifacts:
    reports:
      sast: trivy-*-report.sarif
    expire_in: 1 week
  cache:
    key: trivy-cache
    paths:
      - .trivycache/

# SAST with SonarQube
sonarqube-check:
  stage: security-scan
  image: 
    name: sonarsource/sonar-scanner-cli:latest
    entrypoint: [""]
  variables:
    SONAR_USER_HOME: "${CI_PROJECT_DIR}/.sonar"
    GIT_DEPTH: "0"
  cache:
    key: "${CI_JOB_NAME}"
    paths:
      - .sonar/cache
  script:
    - sonar-scanner
      -Dsonar.projectKey=$SONAR_PROJECT_KEY
      -Dsonar.sources=packages/
      -Dsonar.host.url=$SONAR_HOST_URL
      -Dsonar.login=$SONAR_TOKEN
      -Dsonar.qualitygate.wait=true
      -Dsonar.javascript.lcov.reportPaths=coverage/lcov.info
  allow_failure: false
  only:
    - merge_requests
    - main

# Stage 3: Comprehensive Testing
unit-tests:
  stage: test
  image: node:${NODE_VERSION}-alpine
  script:
    - npm ci --cache .npm --prefer-offline
    - npm run test:unit -- --coverage --watchAll=false
  coverage: '/All files[^|]*\|[^|]*\s+([\d\.]+)/'
  artifacts:
    reports:
      junit: junit.xml
      coverage_report:
        coverage_format: cobertura
        path: coverage/cobertura-coverage.xml
    paths:
      - coverage/
    expire_in: 1 week

integration-tests:
  stage: test
  image: node:${NODE_VERSION}-alpine
  services:
    - name: redis:7-alpine
      alias: redis
    - name: postgres:15-alpine
      alias: postgres
  variables:
    POSTGRES_DB: xaheen_test
    POSTGRES_USER: test
    POSTGRES_PASSWORD: test
    REDIS_URL: redis://redis:6379
    DATABASE_URL: postgresql://test:test@postgres:5432/xaheen_test
  script:
    - npm ci --cache .npm --prefer-offline
    - npm run test:integration
    - npm run test:e2e
  artifacts:
    reports:
      junit: test-results/integration.xml
    expire_in: 1 week

# Performance Testing
performance-tests:
  stage: test
  image: grafana/k6:latest
  script:
    - k6 run --out json=performance-results.json performance-tests/load-test.js
  artifacts:
    reports:
      performance: performance-results.json
    expire_in: 1 week

# Stage 4: Build Application
build:
  stage: build
  image: node:${NODE_VERSION}-alpine
  script:
    - npm ci --cache .npm --prefer-offline
    - npm run build
    - npm run build:production
    - echo "Build completed for version $(node -p "require('./package.json').version")"
  artifacts:
    paths:
      - dist/
      - packages/*/dist/
    expire_in: 1 week
  rules:
    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH
    - if: $CI_PIPELINE_SOURCE == "merge_request_event"

# Stage 5: Container Packaging
docker-build:
  stage: package
  image: docker:24.0.7
  services:
    - docker:24.0.7-dind
  variables:
    DOCKER_TLS_CERTDIR: "/certs"
    DOCKER_BUILDKIT: 1
  before_script:
    - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY
  script:
    - |
      docker build \
        --build-arg NODE_VERSION=${NODE_VERSION} \
        --build-arg BUILD_DATE=$(date -u +'%Y-%m-%dT%H:%M:%SZ') \
        --build-arg VCS_REF=$CI_COMMIT_SHA \
        --build-arg VERSION=$(node -p "require('./package.json').version") \
        --cache-from $CI_REGISTRY_IMAGE:cache \
        --tag $DOCKER_IMAGE_TAG \
        --tag $CI_REGISTRY_IMAGE:latest \
        .
    - docker push $DOCKER_IMAGE_TAG
    - docker push $CI_REGISTRY_IMAGE:latest
  dependencies:
    - build
  rules:
    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH

# Container Security Scanning
container-scan:
  stage: package
  image: 
    name: aquasec/trivy:latest
    entrypoint: [""]
  script:
    - trivy image --exit-code 0 --format template --template "@contrib/sarif.tpl" -o container-scan-report.sarif $DOCKER_IMAGE_TAG
    - trivy image --exit-code 1 --severity HIGH,CRITICAL $DOCKER_IMAGE_TAG
  artifacts:
    reports:
      sast: container-scan-report.sarif
    expire_in: 1 week
  dependencies:
    - docker-build

# Stage 6: Development Deployment
deploy-dev:
  stage: deploy-dev
  image: alpine/helm:3.13.3
  environment:
    name: development
    url: https://dev.xaheen-cli.example.com
  variables:
    KUBE_NAMESPACE: xaheen-cli-dev
    ENVIRONMENT: development
  before_script:
    - apk add --no-cache curl
    - curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
    - chmod +x kubectl && mv kubectl /usr/local/bin/
    - echo $KUBE_CONFIG | base64 -d > kubeconfig
    - export KUBECONFIG=kubeconfig
  script:
    - kubectl create namespace $KUBE_NAMESPACE --dry-run=client -o yaml | kubectl apply -f -
    - |
      helm upgrade --install xaheen-cli-dev ./helm/xaheen-cli \
        --namespace $KUBE_NAMESPACE \
        --set image.repository=$CI_REGISTRY_IMAGE \
        --set image.tag=$CI_COMMIT_REF_SLUG-$CI_COMMIT_SHORT_SHA \
        --set environment=$ENVIRONMENT \
        --set ingress.enabled=true \
        --set ingress.hosts[0].host=dev.xaheen-cli.example.com \
        --set resources.requests.cpu=100m \
        --set resources.requests.memory=128Mi \
        --set resources.limits.cpu=500m \
        --set resources.limits.memory=512Mi \
        --wait --timeout=10m
    - kubectl get pods -n $KUBE_NAMESPACE
  dependencies:
    - docker-build
  rules:
    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH

# Stage 7: Norwegian Compliance Check
norwegian-compliance:
  stage: compliance-check
  image: node:${NODE_VERSION}-alpine
  script:
    - npm ci --cache .npm --prefer-offline
    - npm run compliance:norwegian
    - npm run compliance:gdpr
    - npm run compliance:security-audit
    - echo "Compliance level: $NORWEGIAN_COMPLIANCE_LEVEL"
  artifacts:
    reports:
      junit: compliance-results.xml
    paths:
      - compliance-report.json
    expire_in: 1 month
  dependencies:
    - deploy-dev

# Data Privacy Validation
data-privacy-check:
  stage: compliance-check
  image: node:${NODE_VERSION}-alpine
  script:
    - npm run privacy:scan
    - npm run privacy:validate-data-flow
    - npm run privacy:check-retention-policies
  artifacts:
    paths:
      - privacy-audit-report.json
    expire_in: 1 month

# Stage 8: Staging Deployment
deploy-staging:
  stage: deploy-staging
  image: alpine/helm:3.13.3
  environment:
    name: staging
    url: https://staging.xaheen-cli.example.com
  variables:
    KUBE_NAMESPACE: xaheen-cli-staging
    ENVIRONMENT: staging
  before_script:
    - apk add --no-cache curl
    - curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
    - chmod +x kubectl && mv kubectl /usr/local/bin/
    - echo $KUBE_CONFIG | base64 -d > kubeconfig
    - export KUBECONFIG=kubeconfig
  script:
    - kubectl create namespace $KUBE_NAMESPACE --dry-run=client -o yaml | kubectl apply -f -
    - |
      helm upgrade --install xaheen-cli-staging ./helm/xaheen-cli \
        --namespace $KUBE_NAMESPACE \
        --set image.repository=$CI_REGISTRY_IMAGE \
        --set image.tag=$CI_COMMIT_REF_SLUG-$CI_COMMIT_SHORT_SHA \
        --set environment=$ENVIRONMENT \
        --set ingress.enabled=true \
        --set ingress.hosts[0].host=staging.xaheen-cli.example.com \
        --set resources.requests.cpu=200m \
        --set resources.requests.memory=256Mi \
        --set resources.limits.cpu=1000m \
        --set resources.limits.memory=1Gi \
        --set autoscaling.enabled=true \
        --set autoscaling.minReplicas=2 \
        --set autoscaling.maxReplicas=5 \
        --wait --timeout=15m
    - kubectl get pods -n $KUBE_NAMESPACE
  dependencies:
    - norwegian-compliance
  rules:
    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH
  when: manual

# Stage 9: Security Validation
penetration-testing:
  stage: security-validation
  image: owasp/zap2docker-stable
  script:
    - zap-baseline.py -t https://staging.xaheen-cli.example.com -J zap-report.json
  artifacts:
    paths:
      - zap-report.json
    expire_in: 1 week
  dependencies:
    - deploy-staging
  allow_failure: true

# Runtime Security Monitoring
runtime-security:
  stage: security-validation
  image: falcosecurity/falco:latest
  script:
    - echo "Runtime security monitoring activated"
    - falco --validate-config
  dependencies:
    - deploy-staging

# Stage 10: Production Deployment
deploy-production:
  stage: deploy-production
  image: alpine/helm:3.13.3
  environment:
    name: production
    url: https://xaheen-cli.example.com
  variables:
    KUBE_NAMESPACE: xaheen-cli-prod
    ENVIRONMENT: production
  before_script:
    - apk add --no-cache curl
    - curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
    - chmod +x kubectl && mv kubectl /usr/local/bin/
    - echo $KUBE_CONFIG_PROD | base64 -d > kubeconfig
    - export KUBECONFIG=kubeconfig
  script:
    - kubectl create namespace $KUBE_NAMESPACE --dry-run=client -o yaml | kubectl apply -f -
    # Blue-Green Deployment Strategy
    - |
      if helm list -n $KUBE_NAMESPACE | grep -q xaheen-cli-blue; then
        CURRENT_COLOR=blue
        NEXT_COLOR=green
      else
        CURRENT_COLOR=green
        NEXT_COLOR=blue
      fi
    - echo "Deploying to $NEXT_COLOR environment"
    - |
      helm upgrade --install xaheen-cli-$NEXT_COLOR ./helm/xaheen-cli \
        --namespace $KUBE_NAMESPACE \
        --set image.repository=$CI_REGISTRY_IMAGE \
        --set image.tag=$CI_COMMIT_REF_SLUG-$CI_COMMIT_SHORT_SHA \
        --set environment=$ENVIRONMENT \
        --set color=$NEXT_COLOR \
        --set ingress.enabled=false \
        --set resources.requests.cpu=500m \
        --set resources.requests.memory=512Mi \
        --set resources.limits.cpu=2000m \
        --set resources.limits.memory=2Gi \
        --set autoscaling.enabled=true \
        --set autoscaling.minReplicas=3 \
        --set autoscaling.maxReplicas=10 \
        --set podDisruptionBudget.enabled=true \
        --wait --timeout=20m
    # Health Check
    - kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=xaheen-cli,color=$NEXT_COLOR -n $KUBE_NAMESPACE --timeout=300s
    # Switch Traffic (Blue-Green)
    - kubectl patch service xaheen-cli-service -n $KUBE_NAMESPACE -p '{"spec":{"selector":{"color":"'$NEXT_COLOR'"}}}'
    - kubectl patch ingress xaheen-cli-ingress -n $KUBE_NAMESPACE -p '{"spec":{"rules":[{"host":"xaheen-cli.example.com","http":{"paths":[{"path":"/","pathType":"Prefix","backend":{"service":{"name":"xaheen-cli-'$NEXT_COLOR'","port":{"number":80}}}}]}}]}}'
    # Cleanup old deployment after 5 minutes
    - sleep 300
    - helm uninstall xaheen-cli-$CURRENT_COLOR -n $KUBE_NAMESPACE || true
  dependencies:
    - penetration-testing
  rules:
    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH
  when: manual

# Canary Deployment (Alternative Production Strategy)
deploy-production-canary:
  stage: deploy-production
  image: alpine/helm:3.13.3
  environment:
    name: production-canary
    url: https://xaheen-cli.example.com
  variables:
    KUBE_NAMESPACE: xaheen-cli-prod
    CANARY_WEIGHT: "10"
  script:
    - echo "Deploying canary with $CANARY_WEIGHT% traffic"
    - |
      helm upgrade --install xaheen-cli-canary ./helm/xaheen-cli \
        --namespace $KUBE_NAMESPACE \
        --set image.repository=$CI_REGISTRY_IMAGE \
        --set image.tag=$CI_COMMIT_REF_SLUG-$CI_COMMIT_SHORT_SHA \
        --set environment=production \
        --set canary.enabled=true \
        --set canary.weight=$CANARY_WEIGHT \
        --set resources.requests.cpu=200m \
        --set resources.requests.memory=256Mi \
        --set resources.limits.cpu=1000m \
        --set resources.limits.memory=1Gi \
        --wait --timeout=10m
  rules:
    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH
  when: manual

# Stage 11: Post-Deployment Validation
smoke-tests:
  stage: post-deployment
  image: node:${NODE_VERSION}-alpine
  script:
    - npm ci --cache .npm --prefer-offline
    - npm run test:smoke -- --env=production
    - npm run test:api-health
  dependencies:
    - deploy-production

# Performance Monitoring
performance-monitoring:
  stage: post-deployment
  image: grafana/k6:latest
  script:
    - k6 run --out json=prod-performance.json performance-tests/production-load-test.js
  artifacts:
    reports:
      performance: prod-performance.json
    expire_in: 1 month
  dependencies:
    - deploy-production

# Rollback Job (Manual trigger)
rollback:
  stage: deploy-production
  image: alpine/helm:3.13.3
  variables:
    KUBE_NAMESPACE: xaheen-cli-prod
  script:
    - echo $KUBE_CONFIG_PROD | base64 -d > kubeconfig
    - export KUBECONFIG=kubeconfig
    - helm rollback xaheen-cli -n $KUBE_NAMESPACE
    - kubectl get pods -n $KUBE_NAMESPACE
  when: manual
  allow_failure: false

# Notification
notify-deployment:
  stage: post-deployment
  image: alpine:latest
  script:
    - apk add --no-cache curl
    - |
      curl -X POST $SLACK_WEBHOOK_URL \
        -H 'Content-type: application/json' \
        --data '{
          "text": "ðŸš€ Xaheen CLI deployed successfully to production",
          "attachments": [{
            "color": "good",
            "fields": [
              {"title": "Version", "value": "'$CI_COMMIT_REF_SLUG-$CI_COMMIT_SHORT_SHA'", "short": true},
              {"title": "Environment", "value": "Production", "short": true},
              {"title": "Pipeline", "value": "'$CI_PIPELINE_URL'", "short": false}
            ]
          }]
        }'
  dependencies:
    - smoke-tests
  rules:
    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH
  when: on_success