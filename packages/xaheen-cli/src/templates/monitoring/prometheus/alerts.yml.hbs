# Prometheus Alerting Rules for {{projectName}}
# Generated by Xaheen CLI - Story 4.3 Monitoring and Observability
# Environment: {{environment}}

groups:
  - name: "{{projectName}}.application.alerts"
    interval: 30s
    rules:
      # Application Health Alerts
      - alert: ApplicationDown
        expr: up{job="{{projectName}}"} == 0
        for: 5m
        labels:
          severity: critical
          service: "{{projectName}}"
          team: devops
        annotations:
          summary: "{{projectName}} application is down"
          description: "{{projectName}} application has been down for more than 5 minutes. Instance: {{`{{ $labels.instance }}`}}"
          runbook_url: "https://runbooks.{{projectName}}.com/ApplicationDown"
          dashboard_url: "{{#if features.includes('grafana')}}https://grafana.{{projectName}}.com/d/{{projectName}}-overview{{/if}}"

      - alert: HighErrorRate
        expr: |
          (
            rate(http_requests_total{job="{{projectName}}", status=~"5.."}[5m]) /
            rate(http_requests_total{job="{{projectName}}"}[5m])
          ) > 0.05
        for: 10m
        labels:
          severity: warning
          service: "{{projectName}}"
          team: development
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{`{{ $value | humanizePercentage }}`}} for {{projectName}} on {{`{{ $labels.instance }}`}}"
          runbook_url: "https://runbooks.{{projectName}}.com/HighErrorRate"

      - alert: CriticalErrorRate
        expr: |
          (
            rate(http_requests_total{job="{{projectName}}", status=~"5.."}[5m]) /
            rate(http_requests_total{job="{{projectName}}"}[5m])
          ) > 0.10
        for: 5m
        labels:
          severity: critical
          service: "{{projectName}}"
          team: development
        annotations:
          summary: "Critical error rate detected"
          description: "Error rate is {{`{{ $value | humanizePercentage }}`}} for {{projectName}} on {{`{{ $labels.instance }}`}}"
          runbook_url: "https://runbooks.{{projectName}}.com/CriticalErrorRate"

      # Performance Alerts
      - alert: HighLatency
        expr: |
          histogram_quantile(0.95, 
            rate(http_request_duration_seconds_bucket{job="{{projectName}}"}[5m])
          ) > 0.5
        for: 10m
        labels:
          severity: warning
          service: "{{projectName}}"
          team: development
        annotations:
          summary: "High response latency"
          description: "95th percentile latency is {{`{{ $value }}s`}} for {{projectName}} on {{`{{ $labels.instance }}`}}"
          runbook_url: "https://runbooks.{{projectName}}.com/HighLatency"

      - alert: CriticalLatency
        expr: |
          histogram_quantile(0.95, 
            rate(http_request_duration_seconds_bucket{job="{{projectName}}"}[5m])
          ) > 2.0
        for: 5m
        labels:
          severity: critical
          service: "{{projectName}}"
          team: development
        annotations:
          summary: "Critical response latency"
          description: "95th percentile latency is {{`{{ $value }}s`}} for {{projectName}} on {{`{{ $labels.instance }}`}}"
          runbook_url: "https://runbooks.{{projectName}}.com/CriticalLatency"

      # Throughput Alerts
      - alert: LowThroughput
        expr: |
          rate(http_requests_total{job="{{projectName}}"}[5m]) < 1
        for: 15m
        labels:
          severity: warning
          service: "{{projectName}}"
          team: development
        annotations:
          summary: "Low request throughput"
          description: "Request rate is {{`{{ $value }}`}} requests/second for {{projectName}} on {{`{{ $labels.instance }}`}}"
          runbook_url: "https://runbooks.{{projectName}}.com/LowThroughput"

      - alert: HighThroughput
        expr: |
          rate(http_requests_total{job="{{projectName}}"}[5m]) > 1000
        for: 10m
        labels:
          severity: warning
          service: "{{projectName}}"
          team: sre
        annotations:
          summary: "Unusually high request throughput"
          description: "Request rate is {{`{{ $value }}`}} requests/second for {{projectName}} on {{`{{ $labels.instance }}`}}"
          runbook_url: "https://runbooks.{{projectName}}.com/HighThroughput"

      {{#if runtime}}
      {{#eq runtime 'node'}}
      # Node.js Specific Alerts
      - alert: NodeJSHighMemoryUsage
        expr: |
          (nodejs_heap_size_used_bytes{job="{{projectName}}"} / nodejs_heap_size_total_bytes{job="{{projectName}}"}) > 0.9
        for: 10m
        labels:
          severity: warning
          service: "{{projectName}}"
          runtime: nodejs
        annotations:
          summary: "High Node.js heap memory usage"
          description: "Heap usage is {{`{{ $value | humanizePercentage }}`}} on {{`{{ $labels.instance }}`}}"
          runbook_url: "https://runbooks.{{projectName}}.com/NodeJSMemoryUsage"

      - alert: NodeJSEventLoopLag
        expr: |
          nodejs_eventloop_lag_seconds{job="{{projectName}}"} > 0.1
        for: 5m
        labels:
          severity: warning
          service: "{{projectName}}"
          runtime: nodejs
        annotations:
          summary: "High Node.js event loop lag"
          description: "Event loop lag is {{`{{ $value }}s`}} on {{`{{ $labels.instance }}`}}"
          runbook_url: "https://runbooks.{{projectName}}.com/NodeJSEventLoopLag"

      - alert: NodeJSActiveHandles
        expr: |
          nodejs_active_handles{job="{{projectName}}"} > 1000
        for: 10m
        labels:
          severity: warning
          service: "{{projectName}}"
          runtime: nodejs
        annotations:
          summary: "High number of active handles in Node.js"
          description: "Active handles count is {{`{{ $value }}`}} on {{`{{ $labels.instance }}`}}"
          runbook_url: "https://runbooks.{{projectName}}.com/NodeJSActiveHandles"
      {{/eq}}
      {{/if}}

  - name: "{{projectName}}.infrastructure.alerts"
    interval: 30s
    rules:
      # Infrastructure Alerts
      - alert: HighCPUUsage
        expr: |
          100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
        for: 10m
        labels:
          severity: warning
          service: infrastructure
          team: sre
        annotations:
          summary: "High CPU usage detected"
          description: "CPU usage is {{`{{ $value }}%`}} on {{`{{ $labels.instance }}`}}"
          runbook_url: "https://runbooks.{{projectName}}.com/HighCPUUsage"

      - alert: CriticalCPUUsage
        expr: |
          100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 95
        for: 5m
        labels:
          severity: critical
          service: infrastructure
          team: sre
        annotations:
          summary: "Critical CPU usage detected"
          description: "CPU usage is {{`{{ $value }}%`}} on {{`{{ $labels.instance }}`}}"
          runbook_url: "https://runbooks.{{projectName}}.com/CriticalCPUUsage"

      - alert: HighMemoryUsage
        expr: |
          (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 80
        for: 10m
        labels:
          severity: warning
          service: infrastructure
          team: sre
        annotations:
          summary: "High memory usage detected"
          description: "Memory usage is {{`{{ $value }}%`}} on {{`{{ $labels.instance }}`}}"
          runbook_url: "https://runbooks.{{projectName}}.com/HighMemoryUsage"

      - alert: CriticalMemoryUsage
        expr: |
          (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 95
        for: 5m
        labels:
          severity: critical
          service: infrastructure
          team: sre
        annotations:
          summary: "Critical memory usage detected"
          description: "Memory usage is {{`{{ $value }}%`}} on {{`{{ $labels.instance }}`}}"
          runbook_url: "https://runbooks.{{projectName}}.com/CriticalMemoryUsage"

      - alert: DiskSpaceLow
        expr: |
          (node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"}) * 100 < 20
        for: 5m
        labels:
          severity: warning
          service: infrastructure
          team: sre
        annotations:
          summary: "Low disk space"
          description: "Disk space is {{`{{ $value }}%`}} available on {{`{{ $labels.instance }}`}}"
          runbook_url: "https://runbooks.{{projectName}}.com/DiskSpaceLow"

      - alert: DiskSpaceCritical
        expr: |
          (node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"}) * 100 < 10
        for: 2m
        labels:
          severity: critical
          service: infrastructure
          team: sre
        annotations:
          summary: "Critical disk space"
          description: "Disk space is {{`{{ $value }}%`}} available on {{`{{ $labels.instance }}`}}"
          runbook_url: "https://runbooks.{{projectName}}.com/DiskSpaceCritical"

      - alert: HighDiskIOWait
        expr: |
          irate(node_cpu_seconds_total{mode="iowait"}[5m]) * 100 > 10
        for: 10m
        labels:
          severity: warning
          service: infrastructure
          team: sre
        annotations:
          summary: "High disk I/O wait time"
          description: "I/O wait time is {{`{{ $value }}%`}} on {{`{{ $labels.instance }}`}}"
          runbook_url: "https://runbooks.{{projectName}}.com/HighDiskIOWait"

      - alert: HighNetworkTraffic
        expr: |
          rate(node_network_receive_bytes_total{device!="lo"}[5m]) + rate(node_network_transmit_bytes_total{device!="lo"}[5m]) > 100000000  # 100MB/s
        for: 10m
        labels:
          severity: warning
          service: infrastructure
          team: sre
        annotations:
          summary: "High network traffic"
          description: "Network traffic is {{`{{ $value | humanize }}B/s`}} on {{`{{ $labels.instance }}`}} interface {{`{{ $labels.device }}`}}"
          runbook_url: "https://runbooks.{{projectName}}.com/HighNetworkTraffic"

  {{#if features.includes('blackbox-exporter')}}
  - name: "{{projectName}}.endpoint.alerts"
    interval: 30s
    rules:
      # Endpoint Monitoring Alerts
      - alert: EndpointDown
        expr: |
          probe_success{job="blackbox-http"} == 0
        for: 5m
        labels:
          severity: critical
          service: endpoint-monitoring
          team: sre
        annotations:
          summary: "Endpoint is down"
          description: "Endpoint {{`{{ $labels.instance }}`}} has been down for more than 5 minutes"
          runbook_url: "https://runbooks.{{projectName}}.com/EndpointDown"

      - alert: EndpointHighLatency
        expr: |
          probe_duration_seconds{job="blackbox-http"} > 5
        for: 10m
        labels:
          severity: warning
          service: endpoint-monitoring
          team: sre
        annotations:
          summary: "Endpoint high latency"
          description: "Endpoint {{`{{ $labels.instance }}`}} has latency of {{`{{ $value }}s`}}"
          runbook_url: "https://runbooks.{{projectName}}.com/EndpointHighLatency"

      - alert: SSLCertificateExpiry
        expr: |
          (probe_ssl_earliest_cert_expiry - time()) / 86400 < 30
        for: 1h
        labels:
          severity: warning
          service: ssl-monitoring
          team: sre
        annotations:
          summary: "SSL certificate expiring soon"
          description: "SSL certificate for {{`{{ $labels.instance }}`}} will expire in {{`{{ $value }}`}} days"
          runbook_url: "https://runbooks.{{projectName}}.com/SSLCertificateExpiry"

      - alert: SSLCertificateExpiryCritical
        expr: |
          (probe_ssl_earliest_cert_expiry - time()) / 86400 < 7
        for: 1h
        labels:
          severity: critical
          service: ssl-monitoring
          team: sre
        annotations:
          summary: "SSL certificate expiring soon"
          description: "SSL certificate for {{`{{ $labels.instance }}`}} will expire in {{`{{ $value }}`}} days"
          runbook_url: "https://runbooks.{{projectName}}.com/SSLCertificateExpiry"
  {{/if}}

  {{#if kubernetes}}
  - name: "{{projectName}}.kubernetes.alerts"
    interval: 30s
    rules:
      # Kubernetes Alerts
      - alert: KubernetesPodCrashLooping
        expr: |
          rate(kube_pod_container_status_restarts_total[15m]) > 0
        for: 5m
        labels:
          severity: warning
          service: kubernetes
          team: sre
        annotations:
          summary: "Pod is crash looping"
          description: "Pod {{`{{ $labels.pod }}`}} in namespace {{`{{ $labels.namespace }}`}} is crash looping"
          runbook_url: "https://runbooks.{{projectName}}.com/KubernetesPodCrashLooping"

      - alert: KubernetesPodNotReady
        expr: |
          kube_pod_status_ready{condition="false"} == 1
        for: 10m
        labels:
          severity: warning
          service: kubernetes
          team: sre
        annotations:
          summary: "Pod is not ready"
          description: "Pod {{`{{ $labels.pod }}`}} in namespace {{`{{ $labels.namespace }}`}} is not ready"
          runbook_url: "https://runbooks.{{projectName}}.com/KubernetesPodNotReady"

      - alert: KubernetesNodeNotReady
        expr: |
          kube_node_status_condition{condition="Ready", status="false"} == 1
        for: 5m
        labels:
          severity: critical
          service: kubernetes
          team: sre
        annotations:
          summary: "Kubernetes node is not ready"
          description: "Node {{`{{ $labels.node }}`}} is not ready"
          runbook_url: "https://runbooks.{{projectName}}.com/KubernetesNodeNotReady"

      - alert: KubernetesDeploymentReplicasMismatch
        expr: |
          kube_deployment_spec_replicas != kube_deployment_status_available_replicas
        for: 10m
        labels:
          severity: warning
          service: kubernetes
          team: sre
        annotations:
          summary: "Deployment replicas mismatch"
          description: "Deployment {{`{{ $labels.deployment }}`}} in namespace {{`{{ $labels.namespace }}`}} has {{`{{ $labels.spec_replicas }}`}} desired but {{`{{ $labels.available_replicas }}`}} available replicas"
          runbook_url: "https://runbooks.{{projectName}}.com/KubernetesDeploymentReplicasMismatch"

      - alert: KubernetesHPAScaleCapability
        expr: |
          kube_hpa_status_current_replicas >= kube_hpa_spec_max_replicas
        for: 15m
        labels:
          severity: warning
          service: kubernetes
          team: sre
        annotations:
          summary: "HPA has reached maximum scale"
          description: "HPA {{`{{ $labels.hpa }}`}} in namespace {{`{{ $labels.namespace }}`}} has reached maximum scale of {{`{{ $value }}`}} replicas"
          runbook_url: "https://runbooks.{{projectName}}.com/KubernetesHPAScaleCapability"
  {{/if}}

  {{#if features.includes('custom-metrics')}}
  - name: "{{projectName}}.business.alerts"
    interval: 60s
    rules:
      # Business Logic Alerts
      {{#each prometheus.alertingRules}}
      - alert: {{name}}
        expr: {{expr}}
        for: {{for}}
        labels:
          severity: {{severity}}
          service: "{{../projectName}}"
          {{#each labels}}
          {{@key}}: "{{this}}"
          {{/each}}
        annotations:
          summary: "{{summary}}"
          description: "{{description}}"
          {{#each annotations}}
          {{@key}}: "{{this}}"
          {{/each}}
      {{/each}}
  {{/if}}

  # SLI/SLO Alerts
  {{#if features.includes('sli-slo')}}
  - name: "{{projectName}}.slo.alerts"
    interval: 60s
    rules:
      # Availability SLO (99.9%)
      - alert: SLOAvailabilityBreach
        expr: |
          (
            avg_over_time(up{job="{{projectName}}"}[1h]) < 0.999
          )
        for: 5m
        labels:
          severity: critical
          service: "{{projectName}}"
          slo: availability
          team: sre
        annotations:
          summary: "SLO availability breach detected"
          description: "Availability SLO (99.9%) has been breached. Current availability: {{`{{ $value | humanizePercentage }}`}}"
          runbook_url: "https://runbooks.{{projectName}}.com/SLOAvailabilityBreach"

      # Latency SLO (95% < 500ms)
      - alert: SLOLatencyBreach
        expr: |
          (
            histogram_quantile(0.95, 
              rate(http_request_duration_seconds_bucket{job="{{projectName}}"}[5m])
            ) > 0.5
          )
        for: 10m
        labels:
          severity: warning
          service: "{{projectName}}"
          slo: latency
          team: development
        annotations:
          summary: "SLO latency breach detected"
          description: "Latency SLO (95% < 500ms) has been breached. Current p95: {{`{{ $value }}s`}}"
          runbook_url: "https://runbooks.{{projectName}}.com/SLOLatencyBreach"

      # Error Rate SLO (< 1%)
      - alert: SLOErrorRateBreach
        expr: |
          (
            rate(http_requests_total{job="{{projectName}}", status=~"5.."}[5m]) /
            rate(http_requests_total{job="{{projectName}}"}[5m])
          ) > 0.01
        for: 10m
        labels:
          severity: warning
          service: "{{projectName}}"
          slo: error-rate
          team: development
        annotations:
          summary: "SLO error rate breach detected"
          description: "Error rate SLO (< 1%) has been breached. Current error rate: {{`{{ $value | humanizePercentage }}`}}"
          runbook_url: "https://runbooks.{{projectName}}.com/SLOErrorRateBreach"
  {{/if}}