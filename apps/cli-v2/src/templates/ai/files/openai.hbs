import OpenAI from 'openai';

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

export interface ChatCompletionOptions {
  model?: string;
  temperature?: number;
  maxTokens?: number;
  systemPrompt?: string;
}

export async function generateChatCompletion(
  messages: OpenAI.Chat.ChatCompletionMessageParam[],
  options: ChatCompletionOptions = {}
) {
  const {
    model = 'gpt-4-turbo-preview',
    temperature = 0.7,
    maxTokens = 1000,
    systemPrompt
  } = options;

  const allMessages: OpenAI.Chat.ChatCompletionMessageParam[] = [];
  
  if (systemPrompt) {
    allMessages.push({ role: 'system', content: systemPrompt });
  }
  
  allMessages.push(...messages);

  const completion = await openai.chat.completions.create({
    model,
    messages: allMessages,
    temperature,
    max_tokens: maxTokens,
  });

  return completion.choices[0]?.message?.content || '';
}

export async function generateEmbedding(text: string) {
  const response = await openai.embeddings.create({
    model: 'text-embedding-3-small',
    input: text,
  });

  return response.data[0].embedding;
}

export async function generateImage(prompt: string, options?: {
  size?: '1024x1024' | '1792x1024' | '1024x1792';
  quality?: 'standard' | 'hd';
  n?: number;
}) {
  const response = await openai.images.generate({
    model: 'dall-e-3',
    prompt,
    size: options?.size || '1024x1024',
    quality: options?.quality || 'standard',
    n: options?.n || 1,
  });

  return response.data;
}

export async function streamChatCompletion(
  messages: OpenAI.Chat.ChatCompletionMessageParam[],
  options: ChatCompletionOptions = {}
) {
  const {
    model = 'gpt-4-turbo-preview',
    temperature = 0.7,
    maxTokens = 1000,
  } = options;

  const stream = await openai.chat.completions.create({
    model,
    messages,
    temperature,
    max_tokens: maxTokens,
    stream: true,
  });

  return stream;
}